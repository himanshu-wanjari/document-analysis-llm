# Predictive Keyboard Model using LSTM and Transformers

This project replicates the behavior of a predictive typing keyboard by training a deep learning model (LSTM/Transformer) to suggest the next word based on previous input. It simulates how smartphones predict text input using large language models.

## ğŸ’¡ What It Does

- Predicts the next word in a sentence based on input context
- Handles variable-length sequences using padding and truncation
- Uses tokenization, embedding layers, and sequential models

## âš™ï¸ Model Details

- LSTM-based RNN model using Keras
- Transformer model using HuggingFace or TensorFlow's Attention layers
- Trained on public text datasets (e.g., Gutenberg, Wikipedia dumps)

## ğŸ“š Tech Stack

- Python (TensorFlow/Keras, HuggingFace Transformers, NLTK)
- Jupyter Notebook / Colab
- Matplotlib for visualization

## ğŸ“Š Dataset

- Text corpus (cleaned and tokenized)
- Vocabulary built using frequency thresholds

## ğŸ“‚ Project Structure

